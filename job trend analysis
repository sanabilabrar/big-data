import time
import random
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

def scrape_indeed_jobs(query, location, pages=1):
    options = uc.ChromeOptions()
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36")

    driver = uc.Chrome(options=options)
    job_list = []
    base_url = "https://www.indeed.com"
    search_url = f"{base_url}/jobs?q={query}&l={location}"

    for page in range(pages):
        driver.get(f"{search_url}&start={page * 10}")
        time.sleep(3)

        if "captcha" in driver.page_source.lower() or "just a moment" in driver.title.lower():
            print("CAPTCHA or Cloudflare interstitial detected.")
            print("URL:", driver.current_url)
            input("Solve CAPTCHA manually in browser and press Enter to continue...")
            time.sleep(5)

        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)

        try:
            WebDriverWait(driver, 20).until(
                EC.presence_of_all_elements_located((By.CLASS_NAME, 'job_seen_beacon'))
            )
        except:
            print("Jobs did not load.")
            continue

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        job_cards = soup.find_all('div', class_='job_seen_beacon')
        print(f"[INFO] Found {len(job_cards)} job cards on page {page+1}")

        for card in job_cards:
            title_elem = card.select_one('h2.jobTitle span')
            title = title_elem['title'] if title_elem and title_elem.has_attr('title') else (title_elem.get_text(strip=True) if title_elem else 'N/A')

            company_elem = card.find('span', class_='companyName')
            company = company_elem.get_text(strip=True) if company_elem else 'N/A'

            location_elem = card.select_one('.companyLocation')
            if location_elem:
                location_text = location_elem.get_text(strip=True)
            else:
                alt_loc = card.select_one('div[data-testid="text-location"]')
                location_text = alt_loc.get_text(strip=True) if alt_loc else 'Unknown'

            summary_elem = card.find('div', class_='job-snippet')
            summary = summary_elem.get_text(separator=' ', strip=True) if summary_elem else 'N/A'

            job_list.append({
                'Title': title,
                'Company': company,
                'Location': location_text,
                'Summary': summary
            })

        time.sleep(random.randint(8, 12))

    driver.quit()
    return pd.DataFrame(job_list) if job_list else pd.DataFrame(columns=['Title', 'Company', 'Location', 'Summary'])

def analyze_and_visualize(df):
    if df.empty:
        print("No data available for analysis.")
        return

    df = df[~df['Location'].str.lower().isin(['n/a', '', 'none', 'nan'])]
    df = df[~df['Title'].str.lower().isin(['n/a', '', 'none', 'nan'])]

    if df.empty:
        print("Data after cleaning is empty.")
        return

    df.to_csv("job_trends_cleaned.csv", index=False)

    plt.figure(figsize=(10, 6))
    sns.countplot(y='Location', data=df,
                  order=df['Location'].value_counts().iloc[:10].index,
                  hue='Location', legend=False, palette='Blues_r')
    plt.title('Top 10 Job Locations')
    plt.xlabel('Number of Jobs')
    plt.ylabel('Location')
    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(10, 6))
    sns.countplot(y='Title', data=df,
                  order=df['Title'].value_counts().iloc[:10].index,
                  hue='Title', legend=False, palette='Oranges_r')
    plt.title('Top 10 Job Titles')
    plt.xlabel('Number of Jobs')
    plt.ylabel('Job Title')
    plt.tight_layout()
    plt.show()

def main():
    query = input("Enter job title: ").strip()
    location = input("Enter location: ").strip()
    print(f"Scraping job data for '{query}' in '{location}'...")
    df = scrape_indeed_jobs(query.replace(' ', '+'), location.replace(' ', '+'), pages=1)

    if not df.empty:
        print(f"\nScraped {len(df)} job postings.")
        analyze_and_visualize(df)
    else:
        print("\nNo data scraped.")

if _name_ == "_main_":
    main()
